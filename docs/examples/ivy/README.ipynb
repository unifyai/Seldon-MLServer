{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving a custom model\n",
    "\n",
    "The `mlserver` package comes with inference runtime implementations for `scikit-learn` and `xgboost` models.\n",
    "However, some times we may also need to roll out our own inference server, with custom logic to perform inference.\n",
    "To support this scenario, MLServer makes it really easy to create your own extensions, which can then be containerised and deployed in a production environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this example, we will train a [`ivy` model](https://unify.ai/). \n",
    "`Ivy` unifies all ML frameworks ðŸ’¥ enabling you not only to write code that can be used with any of these frameworks as the backend, but also to convert ðŸ”„ any function, model or library written in any of them to your preferred framework!\n",
    "\n",
    "Out of the box, `mlserver` doesn't provide an inference runtime for `ivy`.\n",
    "However, through this example we will see how easy is to develop our own."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The first step will be to train our model.\n",
    "This will be a very simple feedforward neural network model, based on an example provided in the [`ivy` readme](https://github.com/unifyai/ivy#ivy-as-a-framework)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 --- Loss: 1.62876\n",
      "Epoch: 20 --- Loss: 1.61579\n",
      "Epoch: 30 --- Loss: 1.61942\n",
      "Epoch: 40 --- Loss: 1.76295\n",
      "Epoch: 50 --- Loss: 1.56649\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "# Original source code and more details can be found in:\n",
    "# https://github.com/unifyai/ivy#ivy-as-a-framework\n",
    "\n",
    "import ivy\n",
    "\n",
    "class Regressor(ivy.Module):\n",
    "    def __init__(self, input_dim, output_dim, is_training=True):\n",
    "        self.linear = ivy.Linear(input_dim, output_dim)\n",
    "        self.dropout = ivy.Dropout(0.5, training=is_training)\n",
    "        ivy.Module.__init__(self)\n",
    "\n",
    "    def _forward(self, x, ):\n",
    "        x = ivy.sigmoid(self.linear(x))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "ivy.set_backend('torch')  # set backend to PyTorch\n",
    "\n",
    "model = Regressor(input_dim=3, output_dim=1)\n",
    "optimizer = ivy.Adam(1e-4)\n",
    "\n",
    "# generate some random data\n",
    "x = ivy.random.random_normal(shape=(100, 3))\n",
    "y = ivy.random.random_normal(shape=(100, 1))\n",
    "\n",
    "def loss_fn(pred, target):\n",
    "    return ivy.mean((pred - target)**2)\n",
    "\n",
    "for epoch in range(50):\n",
    "    # forward pass\n",
    "    pred = model(x)\n",
    "\n",
    "    # compute loss and gradients\n",
    "    loss, grads = ivy.execute_with_gradients(lambda v: loss_fn(pred, y), model.v)\n",
    "\n",
    "    # update parameters\n",
    "    model.v = optimizer.step(model.v, grads)\n",
    "\n",
    "    # print current loss\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch: {epoch + 1:2d} --- Loss: {ivy.to_numpy(loss).item():.5f}')\n",
    "        \n",
    "print('Finished training!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving our trained model\n",
    "\n",
    "Now that we have _trained_ our model, the next step will be to save it so that it can be loaded afterwards at serving-time.\n",
    "We will need to save both the weights and model code.\n",
    "\n",
    "This will get saved in a `artifacts` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_dir = \"./artifacts\"\n",
    "ivy_model = \"weights.pkl\"\n",
    "model_file = os.path.join(model_dir, ivy_model)\n",
    "\n",
    "if not os.path.exists(model_dir): os.makedirs(model_dir)\n",
    "model.v.cont_to_disk_as_pickled(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing artifacts/regressor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile artifacts/regressor.py\n",
    "import ivy\n",
    "\n",
    "class Regressor(ivy.Module):\n",
    "    def __init__(self, input_dim, output_dim, is_training=True):\n",
    "        self.linear = ivy.Linear(input_dim, output_dim)\n",
    "        self.dropout = ivy.Dropout(0.5, training=is_training)\n",
    "        ivy.Module.__init__(self)\n",
    "\n",
    "    def _forward(self, x, ):\n",
    "        x = ivy.sigmoid(self.linear(x))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 1\n",
    "backend = 'torch'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving\n",
    "\n",
    "The next step will be to serve our model using `mlserver`. \n",
    "For that, we will first implement an extension which serve as the _runtime_ to perform inference using our custom `ivy` model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom inference runtime\n",
    "\n",
    "Our custom inference wrapper should be responsible of:\n",
    "\n",
    "- Loading the model we saved previously.\n",
    "- Running inference using our loaded model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load models.py\n",
    "import ivy\n",
    "import numpy as np\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "from mlserver.utils import get_model_uri\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "IVY_MODEL = 'regressor.py'\n",
    "IVY_WEIGHTS = \"weights.pkl\"\n",
    "\n",
    "class IvyModel(MLModel):\n",
    "    async def load(self) -> bool:\n",
    "        model_uri = await get_model_uri(self._settings)\n",
    "        model_file = os.path.join(model_uri, IVY_MODEL)\n",
    "        model_weights = os.path.join(model_uri, IVY_WEIGHTS)\n",
    "        spec = importlib.util.spec_from_file_location('models', model_file)\n",
    "        custom_module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(custom_module)\n",
    "        ivy.set_backend(custom_module.backend)\n",
    "        self._model = custom_module.Regressor(custom_module.input_dim, \n",
    "                                              custom_module.output_dim,\n",
    "                                              is_training=False)\n",
    "        self._model.v = ivy.Container.cont_from_disk_as_pickled(model_weights)\n",
    "        return True\n",
    "\n",
    "    @decode_args\n",
    "    async def predict(\n",
    "        self,\n",
    "        X: Optional[np.ndarray] = None,\n",
    "    ) -> np.ndarray:\n",
    "        X = ivy.array(X)\n",
    "        result = self._model(X).data.detach().cpu().numpy()\n",
    "        return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings files\n",
    "\n",
    "The next step will be to create 2 configuration files: \n",
    "\n",
    "- `settings.json`: holds the configuration of our server (e.g. ports, log level, etc.).\n",
    "- `model-settings.json`: holds the configuration of our model (e.g. input type, runtime to use, etc.)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `settings.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load settings.json\n",
    "{\n",
    "    \"debug\": \"true\"\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `model-settings.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model-settings.json\n",
    "{\n",
    "    \"name\": \"ivy-regressor\",\n",
    "    \"implementation\": \"models.IvyModel\",\n",
    "    \"parameters\": {\n",
    "        \"uri\": \"./artifacts\"\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start serving our model\n",
    "\n",
    "Now that we have our config in-place, we can start the server by running `mlserver start .`. This needs to either be ran from the same directory where our config files are or pointing to the folder where they are.\n",
    "\n",
    "```shell\n",
    "mlserver start .\n",
    "```\n",
    "\n",
    "Since this command will start the server and block the terminal, waiting for requests, this will need to be ran in the background on a separate terminal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send test inference request\n",
    "\n",
    "\n",
    "We now have our model being served by `mlserver`.\n",
    "To make sure that everything is working as expected, let's send a request from our test set.\n",
    "\n",
    "For that, we can use the Python types that `mlserver` provides out of box, or we can build our request manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'ivy-regressor',\n",
       " 'id': '15d42bc4-0f81-474e-9416-1b0587850339',\n",
       " 'parameters': {},\n",
       " 'outputs': [{'name': 'output-0',\n",
       "   'shape': [4, 1],\n",
       "   'datatype': 'FP32',\n",
       "   'parameters': {'content_type': 'np'},\n",
       "   'data': [0.5942524671554565,\n",
       "    0.9554225206375122,\n",
       "    0.6363343000411987,\n",
       "    0.26471245288848877]}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from mlserver.types import InferenceRequest\n",
    "from mlserver.codecs import NumpyCodec\n",
    "\n",
    "x_0 = np.random.randn(4,3).astype(np.float32)\n",
    "inference_request = InferenceRequest(\n",
    "    inputs=[\n",
    "        NumpyCodec.encode_input(name=\"X\", payload=x_0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "endpoint = \"http://localhost:8080/v2/models/ivy-regressor/infer\"\n",
    "response = requests.post(endpoint, json=inference_request.dict())\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Now that we have written and tested our custom model, the next step is to deploy it.\n",
    "With that goal in mind, the rough outline of steps will be to first build a custom image containing our code, and then deploy it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying requirements\n",
    "MLServer will automatically find your requirements.txt file and install necessary python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load requirements.txt\n",
    "torch\n",
    "ivy-core\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) install ivy from source instead of pypi as default\n",
    "extremely useful if you're looking to use cutting-edge updates in ivy that aren't updated on the pypi package yet  \n",
    "make sure to comment out ivy-core from requirements.txt file!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following command will generate a Dockerfile where we can add the following bash commands to install ivy from source  \n",
    "```\n",
    "# Install Ivy\n",
    "RUN rm -rf ivy && \\\n",
    "    git clone https://github.com/unifyai/ivy && \\\n",
    "    cd ivy && \\\n",
    "    cat requirements.txt | grep -v \"ivy-\" | pip3 install --no-cache-dir -r /dev/stdin && \\\n",
    "    cat optional.txt | grep -v \"ivy-\" | pip3 install --no-cache-dir -r /dev/stdin && \\\n",
    "    python3 -m pip install --user -e .\n",
    "```\n",
    "note: this has been already done for you in this tutorial (you will find the Dockerfile already exists). feel free to remove the file if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlserver dockerfile ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{note}\n",
    "This section expects that Docker is available and running in the background. \n",
    "```\n",
    "\n",
    "MLServer offers helpers to build a custom Docker image containing your code.\n",
    "In this example, we will use the `mlserver build` subcommand to create an image, which we'll be able to deploy later.\n",
    "\n",
    "\n",
    "Note that this section expects that Docker is available and running in the background, as well as a functional cluster with Seldon Core installed and some familiarity with `kubectl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mlserver build . -t 'my-ivy-server:0.1.0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the image is fully functional, we can spin up a container and then send a test request. To start the container, you can run something along the following lines in a separate terminal:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8080:8080 my-ivy-server:0.1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from mlserver.types import InferenceRequest\n",
    "from mlserver.codecs import NumpyCodec\n",
    "\n",
    "x_0 = np.random.randn(4,3).astype(np.float32)\n",
    "inference_request = InferenceRequest(\n",
    "    inputs=[\n",
    "        NumpyCodec.encode_input(name=\"X\", payload=x_0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "endpoint = \"http://localhost:8080/v2/models/ivy-regressor/infer\"\n",
    "response = requests.post(endpoint, json=inference_request.dict())\n",
    "\n",
    "response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we should be able to see, the server running within our Docker image responds as expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying our custom image\n",
    "\n",
    "```{note}\n",
    "This section expects access to a functional Kubernetes cluster with Seldon Core installed and some familiarity with `kubectl`. \n",
    "```\n",
    "\n",
    "Now that we've built a custom image and verified that it works as expected, we can move to the next step and deploy it.\n",
    "There is a large number of tools out there to deploy images.\n",
    "However, for our example, we will focus on deploying it to a cluster running [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/).\n",
    "\n",
    "```{note}\n",
    "Also consider that depending on your Kubernetes installation Seldon Core might expect to get the container image from a public container registry like [Docker hub](https://hub.docker.com/) or [Google Container Registry](https://cloud.google.com/container-registry). For that you need to do an extra step of pushing the container to the registry using `docker tag <image name> <container registry>/<image name>` and `docker push <container registry>/<image name>` and also updating the `image` section of the yaml file to `<container registry>/<image name>`. \n",
    "```\n",
    "\n",
    "For that, we will need to create a `SeldonDeployment` resource which instructs Seldon Core to deploy a model embedded within our custom image and compliant with the [V2 Inference Protocol](https://github.com/kserve/kserve/tree/master/docs/predict-api/v2).\n",
    "This can be achieved by _applying_ (i.e. `kubectl apply`) a `SeldonDeployment` manifest to the cluster, similar to the one below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile seldondeployment.yaml\n",
    "apiVersion: machinelearning.seldon.io/v1\n",
    "kind: SeldonDeployment\n",
    "metadata:\n",
    "  name: ivy-model\n",
    "spec:\n",
    "  protocol: v2\n",
    "  predictors:\n",
    "    - name: default\n",
    "      graph:\n",
    "        name: ivy-regressor\n",
    "        type: MODEL\n",
    "      componentSpecs:\n",
    "        - spec:\n",
    "            containers:\n",
    "              - name: ivy-regressor\n",
    "                image: my-ivy-server:0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
